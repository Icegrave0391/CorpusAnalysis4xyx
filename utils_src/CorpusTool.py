#coding:utf-8
import re
import os
import jieba
from pprint import pprint
import logging
from gensim.summarization import summarize
from gensim.summarization import keywords

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
pardir = os.path.abspath(os.path.join(os.path.dirname('CorpusTool.py'), os.path.pardir)) + '/'
stopwordPath_default = pardir + 'stopwords.txt'
rawPrefix_default = pardir + 'crawl_files/'
outPrefix_default = pardir + 'corpus_files/'

class CorpusTool(object):
    """
    Tool for crawled news text to corpus.
    """
    def __init__(self, stopwordpath=stopwordPath_default, rawPrefix=rawPrefix_default, outPrefix=outPrefix_default):
        self.params = {}
        # params -- stopwords
        fstopword = open(stopwordpath, 'r', encoding='utf-8')
        self.params['stopwords'] = [word[:-1] for word in fstopword.readlines()]
        self.params['rawPrefix'] = rawPrefix
        self.params['outPrefix'] = outPrefix
        self.params['datalist'] = ['BBC', 'chinadaily', 'DW', 'huanqiu', 'NTY', 'renmin', 'sputniknews', 'CNR', 'fangfang' ]
        self.params['pglist'] =   [19   ,      22     ,   1 ,     30   ,  56  ,    27   ,      30      ,  426 ,      1     ]
        # jieba -- addwords
        jieba.add_word("新冠肺炎")
        jieba.add_word("冠性肺炎")
        jieba.add_word("新型冠性肺炎")
        jieba.add_word("新型冠状病毒")
        jieba.add_word("火神山")
        jieba.add_word("雷神山")
        jieba.add_word("方舱医院")
        jieba.add_word("人民日报")
        jieba.add_word("人民网")
        jieba.add_word("卫星通讯社")
        jieba.add_word("谭德塞")
        jieba.add_word("新华网")
        jieba.add_word("新华社")
        jieba.add_word("武汉肺炎")
        jieba.add_word("中国病毒")
        jieba.add_word("武汉病毒")
        jieba.add_word("中国肺炎")
        jieba.add_word("钻石公主号")
        jieba.add_word("华南海鲜市场")
        jieba.add_word("市长")

    def htmlProcess(self, word):
        """
        Generating corpus words from crawled html-form news text
        Input:  string, which represents a sentence of the html-form text.
        Output: string list, which represents the corpus sentence generated by jieba.
        """
        pattern = re.compile(r'[^\u4e00-\u9fa5]')
        result = re.sub(pattern, '', word)
        result = result.replace('（欢迎点击此处订阅新冠病毒疫情中文简报。更多关于疫情的最新消息，欢迎关注时报英文版实时更新报道。）', '')
        result = result.replace('[欢迎点击此处订阅新冠肺炎疫情每日中文简报，或发送邮件至cn.letters@nytimes.com加入订阅。]', '')
        #print('html process result:', result)
        if len(result):
            return ",".join(jieba.cut(result))
        return result

    def CorpusProcess(self):
        """
        Generating all corpus to file paths: 'self.params['outPrefix']/'

        Output: list of corpus
        """
        # get stopwords
        stopwords = self.params['stopwords']
        res_corplist = []
        for i, datastr in enumerate(self.params['datalist']):
            num_pages = self.params['pglist'][i]
            srcpathprefix = self.params['rawPrefix'] + datastr + '/'
            # handle output path
            dstpath = self.params['outPrefix'] + datastr + '.txt'
            fout = open(dstpath, 'w')
            # handle data per page
            news_list = []
            for j in range(num_pages):
                # current a piece of news corpus & lines per page
                curpg_corpus = []
                curnews_lines = []
                idx = str(j+1)
                fpath = srcpathprefix + datastr + 'Page' + idx + '.txt'
                fin = open(fpath, 'r', encoding='utf-8')
                lines = fin.readlines()
                for line in lines:
                    # end of one piece of news
                    if line == '*\n':
                        # print('++++++++++++news end final lines: ', j+1 ,curnews_lines)
                        corp_news_txt = [word for word in curnews_lines if word not in stopwords]
                        if j == 0: # 1st piece for log
                            news_list.append(corp_news_txt)
                        curpg_corpus.append(corp_news_txt)
                        # reset curnews_lines
                        curnews_lines = []
                    curline = []
                    line = re.split('。|，|！|？|：|（|）|；|”|“|《|》|、', line)
                    for lineword in line:
                        tokens = self.htmlProcess(lineword)
                        if len(tokens) > 0:
                            curline += tokens.split(',')
                            # print('cur line: ', curline)
                    if len(curline) > 0:
                        curnews_lines += curline
                # print('========Page corpus:', curpg_corpus)
                pgcorpus_onelist = [' '.join(c) for c in curpg_corpus]
                # file: per line -> per news corpus
                fout.write('\n'.join(pgcorpus_onelist))
                fout.write('\n')
            res_corplist.append(news_list)
        return res_corplist

    def diaryProcess(self):
        doc = []
        diarycorpus = []
        fpath = pardir + 'fangfangdiary.txt'
        fin = open(fpath, 'r', encoding='utf-8')
        pattern = re.compile(r'[^\u4e00-\u9fa5]')
        lines = fin.readlines()
        for line in lines:
            # end of a diary
            if '*' in line:
                doc.append('')
                diarycorpus.append('。. '.join(doc))
                doc = []  # reset
                continue
            sentences = re.split('。|；|？|!', line[:-1])
            tmp = re.sub(pattern, '', line)
            if len(tmp) == 0:
                continue
            for sentence in sentences:
                res = ' '.join(jieba.cut(sentence.strip()))
                if len(res) > 0:
                    doc.append(res)
        diarycorpus.reverse()
        fout = open(pardir + 'diaryabstract.txt', 'w', encoding='utf-8')
        fout.write('>>>>>>>>> Fang Fang\'s Diary: Abstract <<<<<<<<<\n')
        totaldoc = ''
        for i in range(60):
            totaldoc += diarycorpus[i]
        # total abstract
        abstext = summarize(totaldoc, ratio=0.02).replace(' ', '').replace('.', '')
        fout.write(''.join(abstext.split('\n')))
        fout.write('\n\n>>>>>>>>>KEYWORDS<<<<<<<<<\n')
        # keywords
        key = keywords(totaldoc)
        fout.write(' '.join(key.split('\n')))
        # chapter abstract
        fout.write('\n\n>>>>>>>>>Chapter Abstract<<<<<<<<<\n')
        for i in range(60):
            ratio = 500/len(diarycorpus[i])
            text = summarize(diarycorpus[i], ratio=ratio).replace(' ', '').replace('.', '')
            fout.write(str(i+1) + '\n')
            fout.write(text + '\n\n')
        return ''.join(abstext.split('\n')), ' '.join(key.split('\n'))



if __name__ == '__main__':
    tool = CorpusTool()
    tool.CorpusProcess()
    tool.diaryProcess()