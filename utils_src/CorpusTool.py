#coding:utf-8
import re
import os
import jieba

pardir = os.path.abspath(os.path.join(os.path.dirname('CorpusTool.py'), os.path.pardir)) + '/'
stopwordPath_default = pardir + 'stopwords.txt'
rawPrefix_default = pardir + 'crawl_files/'
outPrefix_default = pardir + 'corpus_files/'

class CorpusTool(object):
    """
    Tool for crawled news text to corpus.
    """
    def __init__(self, stopwordpath=stopwordPath_default, rawPrefix=rawPrefix_default, outPrefix=outPrefix_default):
        self.params = {}
        # params -- stopwords
        fstopword = open(stopwordpath, 'r', encoding='utf-8')
        self.params['stopwords'] = [word[:-1] for word in fstopword.readlines()]
        self.params['rawPrefix'] = rawPrefix
        self.params['outPrefix'] = outPrefix
        self.params['datalist'] = ['BBC', 'chinadaily', 'DW', 'huanqiu', 'NTY', 'renmin', 'sputniknews', 'CNR']
        self.params['pglist'] =   [19   ,      22     ,   1 ,     30   ,  56  ,    27   ,      30      ,  426]
        # jieba -- addwords
        jieba.add_word("新冠肺炎")
        jieba.add_word("冠性肺炎")
        jieba.add_word("新型冠性肺炎")
        jieba.add_word("新型冠状病毒")
        jieba.add_word("火神山")
        jieba.add_word("雷神山")
        jieba.add_word("方舱医院")
        jieba.add_word("人民日报")
        jieba.add_word("人民网")
        jieba.add_word("卫星通讯社")
        jieba.add_word("谭德塞")
        jieba.add_word("新华网")
        jieba.add_word("新华社")
        jieba.add_word("武汉肺炎")
        jieba.add_word("中国病毒")
        jieba.add_word("武汉病毒")
        jieba.add_word("中国肺炎")
        jieba.add_word("钻石公主号")
        jieba.add_word("华南海鲜市场")
        jieba.add_word("市长")

    def htmlProcess(self, word):
        """
        Generating corpus words from crawled html-form news text
        Input:  string, which represents a sentence of the html-form text.
        Output: string list, which represents the corpus sentence generated by jieba.
        """
        pattern = re.compile(r'[^\u4e00-\u9fa5]')
        result = re.sub(pattern, '', word)
        result = result.replace('（欢迎点击此处订阅新冠病毒疫情中文简报。更多关于疫情的最新消息，欢迎关注时报英文版实时更新报道。）', '')
        result = result.replace('[欢迎点击此处订阅新冠肺炎疫情每日中文简报，或发送邮件至cn.letters@nytimes.com加入订阅。]', '')
        #print('html process result:', result)
        if len(result):
            return ",".join(jieba.cut(result))
        return result

    def CorpusProcess(self):
        """
        Generating all corpus to file paths: 'self.params['outPrefix']/'
        """
        # get stopwords
        stopwords = self.params['stopwords']
        for i, datastr in enumerate(self.params['datalist']):
            num_pages = self.params['pglist'][i]
            srcpathprefix = self.params['rawPrefix'] + datastr + '/'
            # handle output path
            dstpath = self.params['outPrefix'] + datastr + '.txt'
            fout = open(dstpath, 'w')
            # handle data per page
            for j in range(num_pages):
                # current a piece of news corpus & lines per page
                curpg_corpus = []
                curnews_lines = []
                idx = str(j+1)
                fpath = srcpathprefix + datastr + 'Page' + idx + '.txt'
                fin = open(fpath, 'r', encoding='utf-8')
                lines = fin.readlines()
                for line in lines:
                    # end of one piece of news
                    if line == '*\n':
                        # print('++++++++++++news end final lines: ', j+1 ,curnews_lines)
                        corp_news_txt = [word for word in curnews_lines if word not in stopwords]
                        curpg_corpus.append(corp_news_txt)
                        # reset curnews_lines
                        curnews_lines = []
                    curline = []
                    line = re.split('。|，|！|？|：|（|）|；|”|“|《|》|、', line)
                    for lineword in line:
                        tokens = self.htmlProcess(lineword)
                        if len(tokens) > 0:
                            curline += tokens.split(',')
                            # print('cur line: ', curline)
                    if len(curline) > 0:
                        curnews_lines += curline
                # print('========Page corpus:', curpg_corpus)
                pgcorpus_onelist = [' '.join(c) for c in curpg_corpus]
                fout.write('\n'.join(pgcorpus_onelist))
                fout.write('\n')



if __name__ == '__main__':
    tool = CorpusTool()
    tool.CorpusProcess()
